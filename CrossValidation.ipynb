{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Tools for scaling data, PCA, and standard datasets\n",
    "from sklearn import preprocessing, decomposition, datasets\n",
    "\n",
    "# Tools for tracking learning curves and perform cross validation\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, validation_curve, learning_curve\n",
    "\n",
    "# The k-NN learning algorithm\n",
    "from sklearn.neighbors import KNeighborsClassifier as kNN\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "# One-hot enconding and ordinal encoding\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "\n",
    "# Apply transformations to columns\n",
    "from sklearn.compose import make_column_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pima = pd.read_csv(\"Datasets/diabetes.csv\")\n",
    "pima.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pima.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Outcome` column contains the labels. We use this to construct our sets of training points and training labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pima.drop(columns='Outcome').values\n",
    "y = pima['Outcome'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we count the proportions of positive and negative labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we split the dataset in training set (60%) and test set (40%) using stratification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.4,random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the cross-validated estimates of the risk for different values of $k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbors = range(1,200,20)\n",
    "train_score, val_score = validation_curve(kNN(), X, y, param_name='n_neighbors', param_range=neighbors, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once more, the regions of underfitting and overfitting for the parameter $k$ are clearly seen in the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('k-NN vs. number of neighbors')\n",
    "plt.plot(neighbors, np.mean(val_score, 1), label='Testing accuracy')\n",
    "plt.plot(neighbors, np.mean(train_score, 1), label='Training accuracy')\n",
    "plt.legend()\n",
    "plt.xlabel('Number of neighbors')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation to evaluate performance of a given algorithm\n",
    "The function `cross_val_score()` performs cross validation to estimate the risk of the classifier output by a given algorithm.\n",
    "\n",
    "Here is an example using $5$-fold cross-validation on the entire dataset to evaluate the performance of $21$-NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = kNN(n_neighbors=21)\n",
    "scores = cross_val_score(knn, X, y, cv=20)\n",
    "scores, scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid-search to find best value of parameter for the learning algorithm\n",
    "We can use the function `GridSearch()` to look for the best parameter of an algorithm using the entire dataset.\n",
    "- Repeat 5-fold cross-validation on the entire dataset for each value of the parameter in the grid\n",
    "- Select the parameter with the best cross-validated score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_grid = {'n_neighbors': range(1, 100, 20)}\n",
    "learner = GridSearchCV(estimator=kNN(), param_grid=k_grid, cv=20, return_train_score=True)\n",
    "learner.fit(X, y)\n",
    "learner.best_params_, learner.best_score_ # vars containing the best parameter value and its corresponding cv score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm with the best parameter, $21$-NN, is available in the variable `learner.best_estimator_`, that is `learner.best_estimator_ = kNN(n_neighbors=21)`\n",
    "\n",
    "We repeat the evaluation of this algorithm using 5-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = learner.best_estimator_\n",
    "scores = cross_val_score(model, X, y, cv=20)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nested cross-validation to evaluate performance of a learning algorithm with parameters to tune\n",
    "We saw that cross-validation allows us to use the data for choosing a good value of the parameter. However, we are still left with the problem of estimating the risk of the classifier generated by the algorithm. Nested cross-validation provides a way of estimating the risk of a classifier generated by an algorithm whose parameters are tuned using cross-validation on the training set.\n",
    "\n",
    "In the following example, we:\n",
    "- Run 5-fold cross-validation on the entire dataset\n",
    "- On the training part of each fold, run *internal* 5-fold cross-validation to find the best value of the parameter\n",
    "- Re-train the model on the training part of the outer fold using the optimized parameter\n",
    "- Test the model on the testing part of the outer fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_grid = {'n_neighbors': range(1,100,20)}\n",
    "learner = GridSearchCV(estimator=kNN(), param_grid=k_grid, cv=20) # internal C-V\n",
    "scores = cross_val_score(learner, X, y, cv=20) # external C-V\n",
    "scores, scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the nested cross-validated estimate is $0.72$, while the cross-validated estimate we computed above using grid search on the entire dataset is higher, $0.74$. This discrepancy occurs because the nested CV estimate is statistically more accurate than the cross-validated estimate, which tends to overfit a bit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the dataset\n",
    "Many learning algorithms may work better when the training set is rescaled in certain ways. Note that, in order to avoid contributing to overfitting, these rescalings should not depend on the training examples.\n",
    "\n",
    "We illustrate the most popular rescaling technique on the cancer dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.4,random_state=42,stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `StandardScaler()` function standardizes the values of each feature $i$. If $x_1(i),\\ldots,x_m(i)$ are the values of the $i$-th feature in the dataset $x(1),\\dots,x(m)$, then `StandardScaler()` replaces each value $x_t(i)$ with\n",
    "$$x_t(i)' = \\frac{x_t(i)-\\mu_i}{\\sigma_i}$$\n",
    "where $$\\mu_i = \\frac{1}{m}\\sum_{t=1}^m x_t(i) \\;\\;\\;\\textrm{and}\\;\\;\\; \\sigma_i^2 = \\frac{1}{m}\\sum_{t=1}^m \\bigl(x_t(i)-\\mu_i\\big)^2$$\n",
    "\n",
    "Note that `standard_scaler.fit_transform()` is used to compute $\\mu_i$ and $\\sigma_i$ for each feature $i$ on the training data and then to rescale the training data. The testing data are rescaled using the parameters computed on the training data. Allowing the learner to compute the rescaling parameters using the testing data would imply that the test set is made available (without labels) before the classifier is generated. This is typically not allowed in the statistical learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_scaler = preprocessing.StandardScaler()\n",
    "X_train_standard = standard_scaler.fit_transform(X_train)\n",
    "X_test_standard = standard_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we compute the test set performance with and without rescaling for different values of $k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbors = range(1,8)\n",
    "test_scores = []\n",
    "test_scores_standard = []\n",
    "\n",
    "for k in neighbors:\n",
    "    knn = kNN(n_neighbors=k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    test_scores.append(knn.score(X_test, y_test))\n",
    "    knn.fit(X_train_standard, y_train)\n",
    "    test_scores_standard.append(knn.score(X_test_standard, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the perfomance in both cases shows the benefits of rescaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.title('k-NN vs. number of neighbors')\n",
    "plt.plot(neighbors, test_scores, label='Testing accuracy')\n",
    "plt.plot(neighbors, test_scores_standard, label='Testing accuracy (scaled)')\n",
    "plt.legend()\n",
    "plt.xlabel('Number of neighbors')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now repeat the same exercise use the Pima Indians dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pima.drop(columns='Outcome').values\n",
    "y = pima['Outcome'].values\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.4,random_state=42, stratify=y)\n",
    "standard_scaler = preprocessing.StandardScaler()\n",
    "X_train_standard = standard_scaler.fit_transform(X_train)\n",
    "X_test_standard = standard_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbors = range(1,100,20)\n",
    "test_scores = []\n",
    "test_scores_standard = []\n",
    "\n",
    "for k in neighbors:\n",
    "    knn = kNN(n_neighbors=k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    test_scores.append(knn.score(X_test, y_test))\n",
    "    knn.fit(X_train_standard, y_train)\n",
    "    test_scores_standard.append(knn.score(X_test_standard, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also in this case, we see that rescaling helps boost the test accuracy when $k$ is not chosen optimally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('k-NN vs. number of neighbors')\n",
    "plt.plot(neighbors, test_scores, label='Testing accuracy')\n",
    "plt.plot(neighbors, test_scores_standard, label='Testing accuracy (scaled)')\n",
    "plt.legend()\n",
    "plt.xlabel('Number of neighbors')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature encoding\n",
    "Scikit_learn does not work on datasets with categorical features. Here we show how to transform categorical values into numerical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"Datasets/StudentsPerformance.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset has 5 categorical features. The labels to predict are `math score`, `reading score`, and `writing score`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = data[['math score', 'reading score', 'writing score']]\n",
    "score.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To have a single label to predict, we replace the three `score` labels with their mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average of all test results\n",
    "data[\"mean score\"] = data[['math score','reading score','writing score']].mean(axis = 'rows')\n",
    "\n",
    "# Drop math score, reading score and writing score\n",
    "data = data.drop(columns=['math score', 'reading score', 'writing score'])\n",
    "\n",
    "# Show first 5 rows of new dataframe\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the values of a categorical features are not ranked, then we use **one-hot enconding**. This corresponds to creating a new dummy binary feature for each value in the range of the categorical feature. For instance, for `gender` which takes values in `{'male', 'female'}` we create two dummy features, say `gender_m` and `gender_f`.\n",
    "\n",
    "Then, every occurrence of `gender = 'male'` is replaced by `gender_m = 1` and `gender_f = 0`. Vice versa, every occurrence of `gender = 'female'` is replace by `gender_m = 0` and `gender_f = 1`.\n",
    "\n",
    "Note that one-hot encoding requires a number of dummy variables equal to the cardinality of the range of the categorical feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate OneHotEncoder\n",
    "\n",
    "ohe = OneHotEncoder(sparse_output = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply OneHotEncoder to the gender column \n",
    "\n",
    "ohe.fit_transform(data[['gender']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the result\n",
    "\n",
    "ohe.fit_transform(data[['gender']])[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first 5 rows of the gender column for comparison\n",
    "\n",
    "data['gender'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the values of the categorical features are ranked according to some criterion, then one should use **ordinal encoding** instead of one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique values in the parental level of education column\n",
    "\n",
    "data['parental level of education'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values of `parental level of education` represent degrees of education, which have a natural ranking.\n",
    "\n",
    "Ordinal encoding maps values to positive integers `1,2,...` starting from the lowest position in the ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the order for the level of education \n",
    "\n",
    "education_categories = ['some high school', 'high school', 'some college', \"associate's degree\", \"bachelor's degree\", \"master's degree\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate ordinal encoder\n",
    "\n",
    "oe = OrdinalEncoder(categories = [education_categories])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply ordinal encoder to parental level of education column\n",
    "\n",
    "oe.fit_transform(data[['parental level of education']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create matrix with datapoints and vector of labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(columns='mean score')\n",
    "y = data['mean score']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now convert all categorical variable at once using the function `make_column_transformer`. We use the object `ohe` of type one-hot encoder for all columns but `parental level of education` for which we use the object `oe` of type ordinal encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_transform = make_column_transformer(\n",
    "    (ohe, ['gender', 'race/ethnicity', 'lunch', 'test preparation course']), \n",
    "    (oe, ['parental level of education']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = column_transform.fit_transform(X)\n",
    "\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the number of features has increased from 5 to 12.\n",
    "\n",
    "Here is the resulting dataset which now contains only numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
